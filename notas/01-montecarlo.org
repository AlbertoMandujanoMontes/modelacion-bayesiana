#+TITLE: EST-46115: Modelación Bayesiana
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Integración Monte Carlo~
:REVEAL_PROPERTIES:
# Template uses org export with export option <R B>
# Alternatives: use with citeproc
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_MATHJAX_URL: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Modelación Bayesiana">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+PROPERTY: header-args:R :session intro :exports both :results output org :tangle ../rscripts/01-montecarlo.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022 | Integración Monte Carlo.\\
*Objetivo*: Estudiar integración numérica en el contexto probabilistico. Estudiar,
 en particular, el método Monte Carlo y entender sus bondades y limitaciones en
 el contexto de inferencia Bayesiana. \\
*Lectura recomendada*: Sección 6.1 de citet:Dogucu2021. Una lectura mas técnica
 sobre reglas de cuadratura se puede encontrar en la sección 3.1 de
 citet:Reich2015. Y una buena referencia (técnica) sobre el método Monte Carlo
 lo encuentran en citet:Sanz-Alonso2019.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup --------------------------------------------------
#+end_src

#+begin_src R :exports none :results none
  library(tidyverse)
  library(patchwork)
  library(scales)

  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)

  ## Para el tema de ggplot
  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#por-qué-integrar][¿Por qué integrar?]]
- [[#integración-numérica][Integración numérica]]
  - [[#ejemplo-proporción][Ejemplo: Proporción]]
  - [[#más-de-un-parámetro][Más de un parámetro]]
  - [[#reglas-de-cuadratura][Reglas de cuadratura]]
- [[#integración-monte-carlo][Integración Monte Carlo]]
  - [[#ejemplo-dardos][Ejemplo: Dardos]]
  - [[#propiedades][Propiedades]]
    - [[#teorema-error-monte-carlo][Teorema [Error Monte Carlo]​]]
    - [[#teorema-tlc-para-estimadores-monte-carlo][Teorema [TLC para estimadores Monte Carlo]​]]
    - [[#nota][Nota:]]
    - [[#nota][Nota:]]
    - [[#nota][Nota:]]
  - [[#ejemplo-proporciones][Ejemplo: Proporciones]]
  - [[#ejemplo-sabores-de-helados][Ejemplo: Sabores de helados]]
  - [[#tarea-sabores-de-helados][Tarea: Sabores de helados]]
- [[#extensiones-muestreo-por-importancia][Extensiones: Muestreo por importancia]]
  - [[#propiedades-muestreo-por-importancia][Propiedades: muestreo por importancia]]
  - [[#ejemplo][Ejemplo]]
:END:

* Introducción

En inferencia bayesiana lo que queremos es poder resolver
\begin{align}
\mathbb{E}[f] = \int_{\Theta}^{} f(\theta) \, \pi(\theta | y ) \,  \text{d}\theta\,. 
\end{align}

#+BEGIN_NOTES

Lo que necesitamos es resolver integrales con respecto a la distribución de interés.

#+END_NOTES

#+REVEAL: split
#+ATTR_REVEAL: :frag (appear)
- La pregunta clave (I) es: ¿qué distribución?
- La pregunta clave (II) es: ¿con qué método numérico resuelvo la integral?
- La pregunta clave (III) es: ¿y si no hay método numérico?


* ¿Por qué integrar?

Consideremos de interés estimar la proporción de volumen de la hiper-esfera
contenida en un hiper-cubo unitario conforme aumenta la dimensión del problema.

#+begin_src R :exports code :results none
  distancia_euclideana <- function(u) sqrt(sum(u * u));

  experimento <- function(ndim){
    nsamples <- 1e5; 
    y <- matrix(runif(nsamples * ndim, -0.5, 0.5), nsamples, ndim);
    mean(apply(y, 1, distancia_euclideana) < 0.5)
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/curse-dimensionality.jpeg :exports results :results output graphics file
  tibble(dims = 1:10) |>
    mutate(prob = map_dbl(dims, experimento)) |>
    ggplot(aes(dims, prob)) +
    geom_point() +
    geom_line() +
    sin_lineas +
    scale_x_continuous(breaks=c(1, 3, 5, 7, 9)) +
  xlab("Número de dimensiones") +
  ylab("Volumen relativo")
#+end_src
#+caption: Evolución del volumen relativo de la hiper-esfera circunscrita dentro del hiper-cubo unitario.
#+RESULTS:
[[file:../images/curse-dimensionality.jpeg]]

#+REVEAL: split
#+begin_src R :exports none :results none
  chi.pdf <- function(x, d) {
    x^(d - 1) * exp(-x^2/2) / (2^(d/2 - 1) * gamma(d/2))
  }
#+end_src

#+begin_src R :exports code :results none
  puntos.grafica <- tibble(dims = c(1, 5, 10, 25, 50, 100)) |>
    mutate(points = map(dims, function(dim){
      tibble(x = seq(0, 15, length.out = 1000)) |>
        mutate(y = chi.pdf(x, dim))
    }), dimensions = factor(dims)) 
#+end_src

#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/densidad-esfera.jpeg :exports results :results output graphics file
  puntos.grafica |>
    unnest(points) |>
    ggplot(aes(x, y, group = dimensions, color = dimensions)) +
    geom_line() + sin_leyenda + 
    sin_lineas + xlab("Número de dimensiones") +
    ylab("Densidad")
#+end_src
#+caption: Densidad de los vecinos de la moda para una Normal multivariada estándar $\mathsf{N}(0, I_p)$.
#+RESULTS:
[[file:../images/densidad-esfera.jpeg]]


#+REVEAL: split
Otro detalle interesante de altas dimensiones es la poca intuición
probabilística que tenemos de estos espacios y de lo que es una muestra típica
de una distribución.

Por ejemplo, para $X \sim \mathsf{N}(0,1)$ estamos acostumbrados a asociar la
moda como el valor de mas alta densidad. Lo cual es un error terrible en varias
dimensiones.

#+REVEAL: split
Consideremos un análisis analítico. Por ejemplo, sabemos que si $X \sim \mathsf{N}(0, I_p)$, entonces tenemos que
\begin{align}
\sum_{i = 1}^{p}X_i^2 \sim \chi^2_{p}\,.
\end{align}
Gráfiquemos el valor central de estas variables aleatorias y sus percentiles del
$2.5\%$ y $97.5\%$. Lo que observamos es que una ~muestra típica~ no se comporta
como el promedio de nuestra distirbución, /aka/ el individuo promedio no es tan
común.

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/typical-sets.jpeg :exports results :results output graphics file
  tibble(dim = 2**seq(0, 8)) |>
    mutate(.centro = sqrt(qchisq(.50, dim)),
           .lower = sqrt(qchisq(.025, dim)),
           .upper = sqrt(qchisq(.975, dim))) |>
  ggplot(aes(dim, .centro)) +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = .3, fill = "gray") + 
  geom_line() + geom_point() + sin_lineas +
  scale_x_log10() +
  ylab("Distancia euclideana al centro") +
  xlab("Número de dimensiones")
#+end_src
#+caption: Distancia euclideana de puntos aleatorios de una Gaussiana multivariada al centro de la distribución. Esto ilustra que aunque el centro es el comportamiento promedio, los puntos típicos de una Gaussiana se encuentran lejos. 
#+RESULTS:
[[file:../images/typical-sets.jpeg]]

#+REVEAL: split
Lo que sucede se conoce como el fenómeno de ~concentración de medida~ donde los
puntos de más alta densidad no corresponden a los puntos de mayor volumen
(probabilidad).

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/concentracion-medida.jpeg :exports results :results output graphics file
    tibble(dim = 2**seq(0, 8)) |>
      mutate(.resultados  = map(dim, function(ndim){
               x <- unlist(purrr::rerun(10000, sum(dnorm(rnorm(ndim),log = TRUE))))
               tibble(x = x) |>
                 summarise(.densidad_tip = mean(x),
                           .lower_densidad = quantile(x, .025),
                           .upper_densidad = quantile(x, .975),
                           .densidad_moda = sum(dnorm(rep(0, ndim), log = TRUE)))
             })) |>
      unnest(.resultados) |>
      ggplot(aes(dim, .densidad_tip)) +
      geom_line(aes(y = .densidad_moda), col = 'red') +
      geom_point(aes(y = .densidad_moda), col = 'red') + 
      geom_ribbon(aes(ymin = .lower_densidad, ymax = .upper_densidad), alpha = .3, fill = "gray") + 
      geom_line() + geom_point() + sin_lineas +
      scale_x_log10() +
      ylab("log-Densidad") +
      xlab("Número de dimensiones")

#+end_src
#+caption: En rojo la log-densidad de la moda de una Gaussiana multivariada. En negro la log-densidad de muestras aleatorias de una Gaussiana multivariada. Esto muestra que los elementos con mayor densidad no corresponden a vecindades de mayor volumen. 
#+RESULTS:
[[file:../images/concentracion-medida.jpeg]]

#+REVEAL: split
Esto explica por qué no queremos realizar la aproximación
\begin{align}
\pi(f) \approx f(\theta^\star)\,, \quad \text{ donde }  \quad \theta^\star = \underset{\theta \in \Theta}{\arg \max} \, \pi(\theta)\,. 
\end{align}


* Integración numérica

Recordemos la definición de integrales Riemann:
$$\int_a^b f(x) \text{d} x\,.$$

#+BEGIN_NOTES
La aproximación utilizando una malla de $N$ puntos sería: 
$$\sum_{n=1}^N f(u_n) \Delta u_n.$$

El método útil cuando las integrales se realizan cuando tenemos pocos parámetros. Es decir, $\theta \in \mathbb{R}^p$ con $p$ pequeña. 
#+END_NOTES

#+begin_src R :exports none :results none
  ## Ejemplo de integracion numerica -----------------------

  grid.n          <- 11                 # Número de celdas 
  grid.size       <- 6/(grid.n+1)       # Tamaño de celdas en el intervalo [-3, 3]
  norm.cuadrature <- tibble(x = seq(-3, 3, by = grid.size), y = dnorm(x) )


  norm.density <- tibble(x = seq(-5, 5, by = .01), 
         y = dnorm(x) ) 

#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/quadrature.jpeg :exports results :results output graphics file
  norm.cuadrature |>
    ggplot(aes(x=x + grid.size/2, y=y)) + 
    geom_area(data = norm.density, aes(x = x, y = y), fill = 'lightblue') + 
    geom_bar(stat="identity", alpha = .3) + 
    geom_bar(aes(x = x + grid.size/2, y = -0.01), fill = 'black', stat="identity") + 
    sin_lineas + xlab('') + ylab("") + 
    annotate('text', label = expression(Delta~u[n]),
             x = .01 + 5 * grid.size/2, y = -.02, size = 12) + 
    annotate('text', label = expression(f(u[n]) ),
             x = .01 + 9 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2), size = 12) + 
    annotate('text', label = expression(f(u[n]) * Delta~u[n]), 
             x = .01 + 5 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2)/2, 
             angle = -90, alpha = .7, size = 12) + sin_ejes
#+end_src
#+caption: Integral por medio de discretización con $N = 11$.
#+RESULTS:
[[file:../images/quadrature.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/quadrature-hi.jpeg :exports results :results output graphics file
  grid.n          <- 101                 # Número de celdas 
  grid.size       <- 6/(grid.n+1)       # Tamaño de celdas en el intervalo [-3, 3]
  norm.cuadrature <- tibble(x = seq(-3, 3, by = grid.size), y = dnorm(x) )

  norm.cuadrature |>
      ggplot(aes(x=x + grid.size/2, y=y)) + 
      geom_area(data = norm.density, aes(x = x, y = y), fill = 'lightblue') + 
      geom_bar(stat="identity", alpha = .3) + 
      geom_bar(aes(x = x + grid.size/2, y = -0.01), fill = 'black', stat="identity") + 
      sin_lineas + xlab('') + ylab("") + 
      annotate('text', label = expression(Delta~u[n]),
               x = .01 + 5 * grid.size/2, y = -.02, size = 12) + 
      annotate('text', label = expression(f(u[n]) ),
               x = .01 + 9 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2), size = 12) + 
      annotate('text', label = expression(f(u[n]) * Delta~u[n]), 
               x = .01 + 5 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2)/2, 
               angle = -90, alpha = .7, size = 12) + sin_ejes
#+end_src
#+caption: Integral por medio de una malla fina, $N = 101$. 
#+RESULTS:
[[file:../images/quadrature-hi.jpeg]]


** Ejemplo: Proporción

Supongamos que $p(S_n = k|\theta) \propto \theta^k(1-\theta)^{n-k}$ cuando
observamos $k$ éxitos en $n$ pruebas independientes. Supongamos que nuestra
inicial es $p(\theta) = 2\theta$ (~checa que es una densidad~).

#+REVEAL: split
#+begin_src R :exports code :results none
  crear_log_post <- function(n, k){
    function(theta){
      verosim <- k * log(theta) + (n - k) * log(1 - theta)
      inicial <- log(theta)
      verosim + inicial
    }
  }
#+end_src

#+REVEAL: split
#+begin_src R
  # observamos 3 exitos en 4 pruebas:
  log_post <- crear_log_post(4, 3)
  prob_post <- function(x) { exp(log_post(x))}
  # integramos numericamente
  p_x <- integrate(prob_post, lower = 0, upper = 1, subdivisions = 100L)
  p_x
#+end_src

#+RESULTS:
#+begin_src org
0.033 with absolute error < 3.7e-16
#+end_src

#+REVEAL: split
Y ahora podemos calcular la media posterior:
\begin{align}
\mathbb{E}[\theta | S_n] = \int_{\Theta} \theta \, \pi(\theta | S_n)\, \text{d}\theta\,.
\end{align}

#+begin_src R
      media_funcion <- function(theta){
        theta * prob_post(theta) / p_x$value
      }
      integral_media <- integrate(media_funcion,
                                  lower = 0, upper = 1,
                                  subdivisions = 100L)
      media_post <- integral_media$value 
      c(Numerico = media_post, Analitico = 5/(2+5))
#+end_src

#+RESULTS:
#+begin_src org
 Numerico Analitico 
     0.71      0.71
#+end_src

** Más de un parámetro

#+BEGIN_NOTES
Consideramos ahora un espacio con $\theta \in \mathbb{R}^p$. Si conservamos $N$
puntos por cada dimensión, ¿cuántos puntos en la malla necesitaríamos?  Lo que
tenemos son recursos computacionales limitados y hay que buscar hacer el mejor
uso de ellos. En el ejemplo, hay zonas donde no habrá contribución en la
integral.
#+END_NOTES


#+HEADER: :width 1500 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/eruption-quadrature.jpeg :exports results :results output graphics file
  canvas <- ggplot(faithful, aes(x = eruptions, y = waiting)) +
    xlim(0.5, 6) +
    ylim(40, 110)

  grid.size <- 10 - 1

  mesh <- expand.grid(x = seq(0.5, 6, by = (6-.5)/grid.size),
                      y = seq(40, 110, by = (110-40)/grid.size))

  g1 <- canvas +
    geom_density_2d_filled(aes(alpha = ..level..), bins = 8) +
    scale_fill_manual(values = rev(color.itam)) + 
    sin_lineas + theme(legend.position = "none") +
    geom_point(data = mesh, aes(x = x, y = y)) + 
    annotate("rect", xmin = .5 + 5 * (6-.5)/grid.size, 
             xmax = .5 + 6 * (6-.5)/grid.size, 
             ymin = 40 + 3 * (110-40)/grid.size, 
             ymax = 40 + 4 * (110-40)/grid.size,
             linestyle = 'dashed', 
             fill = 'salmon', alpha = .4) + ylab("") + xlab("") + 
    annotate('text', x = .5 + 5.5 * (6-.5)/grid.size, 
             y = 40 + 3.5 * (110-40)/grid.size, 
             label = expression(u[n]), color = 'red', size = 15) +
    theme(axis.ticks = element_blank(), 
          axis.text = element_blank())


  g2 <- canvas + 
    stat_bin2d(aes(fill = after_stat(density)), binwidth = c((6-.5)/grid.size, (110-40)/grid.size)) +
    sin_lineas + theme(legend.position = "none") +
    theme(axis.ticks = element_blank(), 
          axis.text = element_blank()) +
    scale_fill_distiller(palette = "Greens", direction = 1) + 
    sin_lineas + theme(legend.position = "none") +
    ylab("") + xlab("")

  g3 <- canvas + 
    stat_bin2d(aes(fill = after_stat(density)), binwidth = c((6-.5)/25, (110-40)/25)) +
    sin_lineas + theme(legend.position = "none") +
    theme(axis.ticks = element_blank(), 
          axis.text = element_blank()) +
    scale_fill_distiller(palette = "Greens", direction = 1) + 
    sin_lineas + theme(legend.position = "none") +
    ylab("") + xlab("")

  g1 + g2 + g3
#+end_src
#+caption: Integral por método de malla. 
#+RESULTS:
[[file:../images/eruption-quadrature.jpeg]]

** Reglas de cuadratura

Por el momento hemos escogido aproximar las integrales por medio de una aproximación con una ~malla uniforme~.
Sin embargo, se pueden utilizar aproximaciones 
$$\int_a^b f(x) \text{d} x \approx \sum_{n=1}^N f(\xi_n)\, \omega_n\,.$$

Estas aproximaciones usualmente se realizan para integrales en intervalos cerrados $[a,b]$. La regla de cuadratura determina los pesos $\omega_n$ y los centros $\xi_n$ pues se escogen de acuerdo a ~ciertos criterios de convergencia~.

#+BEGIN_NOTES
Por ejemplo, se consideran polinomios que aproximen con cierto grado de precisión el integrando. Los pesos y los centros se escogen de acuerdo a la familia de polinomios. Pues para cada familia se tienen identificadas las mallas que optimizan la aproximación. Ver sección 3.1 de citet:Reich2015. 
#+END_NOTES

* Integración Monte Carlo
\begin{gather*}
\pi(f) = \mathbb{E}_\pi[f] = \int f(x) \pi(x) \text{d}x\,,\\
\hat{\pi}_N^{\textsf{MC}}(f) = \frac1N \sum_{n = 1}^N f( x^{(n)}), \qquad \text{ donde }  x^{(n)} \overset{\mathsf{iid}}{\sim} \pi, \qquad \text{ con } n = 1, \ldots, N \,, \\
 \pi(f) \approx \hat{\pi}_N^{\textsf{MC}}(f)\,.
\end{gather*} 


** Ejemplo: Dardos

Consideremos el experimento de lanzar dardos uniformemente en un cuadrado de
tamaño 2, el cual contiene un circulo de radio 1.

#+HEADER: :width 1100 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-montecarlo.jpeg :exports results :results output graphics file
  ## Integración Monte Carlo ----------------------------------- 
  genera_dardos <- function(n = 100){
      tibble(x1 = runif(n, min = -1, max = 1), 
             x2 = runif(n, min = -1, max = 1)) |> 
        mutate(resultado = ifelse(x1**2 + x2**2 <= 1., 1., 0.))
    }

    dardos <- tibble(n = seq(2,5)) |> 
      mutate(datos = map(10**n, genera_dardos)) |> 
      unnest() 

    dardos |> 
      ggplot(aes(x = x1, y = x2)) + 
        geom_point(aes(color = factor(resultado))) + 
        facet_wrap(~n, nrow = 1) +  
      sin_lineas + sin_ejes + sin_leyenda + coord_equal()
#+end_src
#+caption: Integración Monte Carlo para aproximar $\pi$. 
#+RESULTS:
[[file:../images/dardos-montecarlo.jpeg]]

#+begin_src R :exports none :results none
  dardos |>
    group_by(n) |>
    summarise(aprox = 4 * mean(resultado)) 
#+end_src

#+RESULTS:
#+begin_src org
  n aprox
1 2   3.1
2 3   3.2
3 4   3.1
4 5   3.1
#+end_src

#+REVEAL: split
Si escogemos $N$ suficientemente grande entonces nuestro promedio converge a la
integral. En [[fig-mc-rolling]] se muestra para cada $n$ en el eje horizontal cómo
cambia nuestra estimación $\hat \pi_n^{\mathsf{MC}}(f)$ .

#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-consistencia.jpeg :exports results :results output graphics file
  set.seed(1087)
  genera_dardos(n = 2**16) |> 
    mutate(n = seq(1, 2**16), 
           approx = cummean(resultado) * 4) |> 
    ggplot(aes(x = n, y = approx)) + 
      geom_line() + 
      geom_hline(yintercept = pi, linetype = 'dashed') + 
      scale_x_continuous(trans='log10', 
                         labels = trans_format("log10", math_format(10^.x))) + 
    ylab('Aproximación') + xlab("Número de muestras") + sin_lineas

#+end_src
#+caption: Estimación $\hat{\pi}_N^{\textsf{MC}}(f)$ con $N \rightarrow \infty$.
#+name: fig-mc-rolling
#+RESULTS:
[[file:../images/dardos-consistencia.jpeg]]

#+REVEAL: split
También podemos en replicar el experimento unas $M$ veces y observar cómo
cambiaría nuestra estimación con distintas semillas. Por ejemplo, podemos
replicar el experimento 10 veces. En ~R~ y ~python~ lo usual es utilizar ~arreglos
multidimensionales~ para poder guardar muestras bajo distintas replicas.

#+begin_src R :exports both :results org
  set.seed(108)
  nsamples <- 10**4; nexp <- 100
  U <- runif(nexp * 2 * nsamples)
  U <- array(U, dim = c(nexp, 2, nsamples))
  apply(U[1:5,,], 1, str)
#+end_src

#+RESULTS:
#+begin_src org
 num [1:2, 1:10000] 0.4551 0.7159 0.164 0.0627 0.5291 ...
 num [1:2, 1:10000] 0.404 0.2313 0.9282 0.0426 0.0883 ...
 num [1:2, 1:10000] 0.351 0.739 0.449 0.658 0.369 ...
 num [1:2, 1:10000] 0.664 0.984 0.627 0.762 0.185 ...
 num [1:2, 1:10000] 0.4635 0.6107 0.0115 0.7251 0.0117 ...
NULL
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  resultados <- apply(U, 1, function(x){
    dardos <- apply(x**2, 2, sum)
    exitos <- ifelse(dardos <= 1, 1, 0)
    prop   <- cummean(exitos)
    4 * prop
  })
#+end_src

#+REVEAL: split
Lo cual nos permite realizar distintos escenarios posibles. 
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-trayectorias.jpeg :exports results :results output graphics file
  resultados |>
    as_data_frame() |>
    mutate(n = 1:nsamples) |>
    pivot_longer(cols = 1:10) |>
    ggplot(aes(n, value)) +
    geom_line(aes(group = name, color = name)) +
    geom_hline(yintercept = pi, linetype = 'dashed') + 
    scale_x_continuous(trans='log10', 
                       labels = trans_format("log10", math_format(10^.x))) + 
    ylab('Aproximación') + xlab("Número de muestras") + sin_lineas + sin_leyenda +
    ylim(0, 7)
#+end_src
#+caption: Réplica de las trayectorias de diversas realizaciones de la aproximación de la integral.
#+RESULTS:
[[file:../images/dardos-trayectorias.jpeg]]

#+REVEAL: split
Bajo ciertas consideraciones teóricas podemos esperar un buen comportamiento de
nuestro estimador de la integral. E incluso podríamos (si el número de
simulaciones lo permite) aproximar dicho comportamiento utilizando
distribuciones asintóticas, ($\mathsf{TLC}$).

#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-normalidad.jpeg :exports results :results output graphics file
      resultados |>
        as_data_frame() |>
        mutate(n = 1:nsamples) |>
        pivot_longer(cols = 1:nexp) |>
        group_by(n) |>
        summarise(promedio = mean(value),
                  desv.est = sd(value),
                  y.lo = promedio - 2 * desv.est,
                  y.hi = promedio + 2 * desv.est) |>
        ggplot(aes(n , promedio)) +
        geom_ribbon(aes(ymin = y.lo, ymax = y.hi), fill = "gray", alpha = .3) +
        geom_ribbon(aes(ymin = promedio - 2 * sqrt(pi * (4 - pi)/(n)),
                        ymax = promedio + 2 * sqrt(pi * (4 - pi)/(n))),
                    fill = "salmon", alpha = .1) +
        geom_hline(yintercept = pi, linetype = 'dashed') + 
        geom_line() +
        scale_x_continuous(trans='log10', 
                           labels = trans_format("log10", math_format(10^.x))) + 
        ylab('Aproximación') + xlab("Número de muestras") + sin_lineas + sin_leyenda +
      ylim(0, 7)
#+end_src
#+caption: Comportamiento promedio e intervalos de confianza. 
#+RESULTS:
[[file:../images/dardos-normalidad.jpeg]]

#+REVEAL: split
Podemos explicar la reducción de los intervalos de confianza por medio de la
varianza de la estimación de la integral en las distintas réplicas que
tenemos. Mas adelante explicaremos de dónde viene la línea punteada. Vemos cómo,
aunque captura bien la reducción en varianza, puede sub- o sobre-estimarla.
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-cota-cramerrao.jpeg :exports results :results output graphics file
  resultados |>
    as_data_frame() |>
    mutate(n = 1:nsamples) |>
    pivot_longer(cols = 1:nexp) |>
    group_by(n) |>
    summarise(varianza = var(value/4)) |>
    mutate(cramer.rao = pi * (4 - pi)/(16 * n)) |>
    ggplot(aes(n , varianza)) +
    geom_line() +
    geom_line(aes(n, cramer.rao), lty = 2, color = 'red') +
    scale_y_continuous(trans='log10') +
    scale_x_continuous(trans='log10', 
                       labels = trans_format("log10", math_format(10^.x))) + 
    ylab('Varianza') + xlab("Número de muestras") + sin_lineas + sin_leyenda
#+end_src
#+caption: Comportamiento promedio e intervalos de confianza. 
#+RESULTS:
[[file:../images/dardos-cota-cramerrao.jpeg]]

** Propiedades

A continuación enunciaremos algunas propiedades clave del método Monte
Carlo. Poco a poco las iremos explicando y en particular discutiremos algunas de
ellas.

*** ~Teorema~ [Error Monte Carlo]
Sea $f : \mathbb{R}^p \rightarrow \mathbb{R}$ cualquier función bien
comportada$^\dagger$.  Entonces, el estimador Monte Carlo es *insesgado*. Es
decir, se satisface

\begin{align}
\mathbb{E}\left[\hat  \pi_N^{\textsf{MC}}(f) - \pi(f)\right] = 0,
\end{align}
para cualquier $N$. Usualmente estudiamos el error en un escenario pesimista
donde medimos el *error cuadrático medio* en el peor escenario

\begin{align*}
\sup_{f \in \mathcal{F}} \, \,  \mathbb{E}\left[ \left(\hat \pi_N^{\textsf{MC}}(f) - \pi(f) \right)^2 \right] \leq \frac1N.
\end{align*}

#+BEGIN_NOTES
Esta desigualdad nos muestra una de las propiedades que usualmente se celebran
de los métodos Monte Carlo. La integral y nuestra aproximación de ella por medio
de simulaciones tiene un error acotado proporcionalmente por el número de
simulaciones.
#+END_NOTES

#+REVEAL: split
En particular, la varianza del estimador (*error estándar*) satisface la igualdad
$$ \textsf{ee}^2\left(\hat \pi_N^{\textsf{MC}}(f)\right) = \frac{\mathbb{V}_\pi( f )}{N}.$$

#+BEGIN_NOTES
Esta igualdad, aunque consistente con nuestra desigualdad anterior, nos dice
algo mas. El error de nuestra aproximación *depende* de la varianza de $f$ bajo la
distribución $\pi$.
#+END_NOTES

*** ~Teorema~ [TLC para estimadores Monte Carlo]
Sea $f$ una función *bien comportada* $^{\dagger\dagger}$, entonces bajo una $N$
suficientemente grande tenemos
\begin{align}
\sqrt{N} \left(\hat \pi_N^{\textsf{MC}} (f) - \pi(f) \right) \sim \mathsf{N}\left(0, \mathbb{V}_\pi(f)\right)\,.
\end{align}

*** ~Nota~:
El estimador Monte Carlo del que hablamos, $\hat \pi_{N}^{\mathsf{MC}}(f)$, es una estimación con una ~muestra finita de simulaciones~. En ese sentido podemos pensar que tenemos un /mapeo/ de muestras a estimador
\begin{align}
(x^{(1)}, \ldots, x^{(N)}) \mapsto  \hat \pi_N^{\mathsf{MC}}(f)\,,
\end{align}
con $x^{(i)} \overset{\mathsf{iid}}{\sim} \pi$ . 

#+REVEAL: split
De lo cual es natural pensar: ¿y si hubiéramos observado otro conjunto de
simulaciones? Nuestro proceso de estimación es el mismo pero la muestra puede
cambiar.

#+REVEAL: split
En este sentido nos preguntamos por el ~comportamiento promedio~ bajo distintas
muestras observadas
\begin{align}
\mathbb{E}[\hat \pi_N^{\mathsf{MC}}(f)] = \mathbb{E}_{x_{1}, \ldots, x_{N}}[\hat \pi_N^{\mathsf{MC}}(f)]\,.
\end{align}
De la misma manera nos podemos preguntar sobre la ~dispersión alrededor de dicho
promedio~ (varianza)
\begin{align}
\mathbb{V}[\hat \pi_N^{\mathsf{MC}}(f)] = \mathbb{V}_{x_{1}, \ldots, x_{N}}[\hat \pi_N^{\mathsf{MC}}(f)]\,.
\end{align}

#+REVEAL: split
Al ser un ejercicio de ~estimación~ la desviación estándar del estimador recibe el
nombre de ~error estándar~. Lo cual denotamos por
\begin{align}
\mathsf{ee}[\hat \pi_N^{\mathsf{MC}}(f)] = \left( \mathbb{V}[\hat \pi_N^{\mathsf{MC}}(f)]  \right)^{1/2}= \left(  \frac{\mathbb{V}_\pi( f )}{N} \right)^{1/2}\,.
\end{align}

*** ~Nota~:
Para algunos estimadores la fórmula del error estándar se puede obtener de
manera analítica. Para otro tipo, tenemos que
utilizar propiedades asintóticas (p.e. cota de Cramer-Rao).

#+REVEAL: split
Hay casos en los que no existe una fórmula asintótica o resultado analítico, pero
podemos usar simulación [ ~8)~ ] para cuantificar dicha dispersión.

*** ~Nota~:
Hay situaciones en las que la distribución normal asintótica no tiene
sentido. Para este tipo de situaciones también veremos cómo podemos utilizar
simulación para cuantificar dicha dispersión.

#+DOWNLOADED: screenshot @ 2022-08-29 19:52:47
#+attr_html: :width 700 :align center
#+caption: Comportamiento promedio e intervalos de confianza con aproximación asintótica.
[[file:../images/dardos-normalidad.jpeg]]

** Ejemplo: Proporciones

Consideramos la estimación de una proporción $\theta$, tenemos como inicial
$p(\theta) \propto \theta$, que es una $\mathsf{Beta}(2,1)$. Si observamos 3
éxitos en 4 pruebas, entonces sabemos que la posterior es $p(\theta|x)\propto
\theta^4(1-\theta)$, que es una $\mathsf{Beta}(5, 2)$. Si queremos calcular la
media y el segundo momento posterior para $\theta$, en teoría necesitamos
calcular
\begin{align}
\mu_1 = \int_0^1 \theta \,\, p(\theta|X = 3)\, \text{d}\theta,\qquad  \mu_2=\int_0^1 \theta^2 \,\, p(\theta|X = 3)\, \text{d}\theta\,.
\end{align}

#+REVEAL: split
#+begin_src R :exports none :results none
  ### Ejemplo proporciones ------------------ 
#+end_src

Utilizando el ~método Monte Carlo~: 
#+begin_src R
theta <- rbeta(10000, 5, 2)
media_post <- mean(theta)
momento_2_post <- mean(theta^2)
c(mu_1 = media_post, mu_2 = momento_2_post)
#+end_src

#+RESULTS:
#+begin_src org
mu_1 mu_2 
0.71 0.54
#+end_src

#+REVEAL: split
Incluso, podemos calcular cosas mas /exóticas/ como
\begin{align}
P(e^{\theta}> 2|x)\,.
\end{align}

#+begin_src R
mean(exp(theta) > 2)
#+end_src

#+RESULTS:
#+begin_src org
[1] 0.61
#+end_src

** Ejemplo: Sabores de helados

Supongamos que probamos el nivel de gusto para 4 sabores distintos de una
paleta. Usamos 4 muestras de aproximadamente 50 personas diferentes para cada
sabor, y cada uno evalúa si le gustó mucho o no. Obtenemos los siguientes
resultados:
#+begin_src R :exports none :results none
  ### Ejemplo helados ------------------------- 
#+end_src

#+begin_src R :exports results
  datos <- tibble(
    sabor = c("fresa", "limon", "mango", "guanabana"),
    n = c(50, 45, 51, 50), gusto = c(36, 35, 42, 29)) |> 
    mutate(prop_gust = gusto / n)

  datos 
#+end_src

#+caption: Resultados de las encuestas.
#+RESULTS:
#+begin_src org
      sabor  n gusto prop_gust
1     fresa 50    36      0.72
2     limón 45    35      0.78
3     mango 51    42      0.82
4 guanábana 50    29      0.58
#+end_src

#+REVEAL: split
Usaremos como inicial $\mathsf{Beta}(2, 1)$ (pues hemos obervado cierto sesgo de
cortesía en la calificación de sabores, y no es tan probable tener valores muy
bajos) para todos los sabores, es decir $p(\theta_i)$ es la funcion de densidad
de una $\mathsf{Beta}(2, 1)$. La inicial conjunta la definimos entonces, usando
~independencia inicial~, como
$$p(\theta_1,\theta_2, \theta_3,\theta_4) = p(\theta_1)p(\theta_2)p(\theta_3)p(\theta_4)\,.$$

#+REVEAL: split
Pues inicialmente establecemos que ningún parámetro da información sobre otro:
saber que mango es muy gustado no nos dice nada acerca del gusto por fresa. Bajo
este supuesto, y el supuesto adicional de que las muestras de cada sabor son
independientes, podemos mostrar que las ~posteriores son independientes~:
$$p(\theta_1,\theta_2,\theta_3, \theta_4|k_1,k_2,k_3,k_4) = p(\theta_4|k_1)p(\theta_4|k_2)p(\theta_4|k_3)p(\theta_4|k_4)$$

#+REVEAL: split
#+begin_src R :exports results
  datos <- datos |>
    mutate(a_post = gusto + 2,
           b_post = n - gusto + 1,
           media_post = a_post/(a_post + b_post))
  datos 
#+end_src

#+caption: Resultado de inferencia Bayesiana. 
#+RESULTS:
#+begin_src org
      sabor  n gusto prop_gust a_post b_post media_post
1     fresa 50    36      0.72     38     15       0.72
2     limón 45    35      0.78     37     11       0.77
3     mango 51    42      0.82     44     10       0.81
4 guanábana 50    29      0.58     31     22       0.58
#+end_src

#+REVEAL: split
Podemos hacer preguntas interesantes como: ¿cuál es la probabilidad de que mango
sea el sabor preferido?  Para contestar esta pregunta podemos utilizar
simulación y responder por medio de un procedimiento Monte Carlo.

#+begin_src R :exports none :results none
  modelo_beta <- function(params, n = 5000){
    rbeta(n, params$alpha, params$beta)
  }
#+end_src

#+begin_src R :exports code :results none
  ## Generamos muestras de la posterior
  paletas <- datos |>
    mutate(alpha = a_post, beta = b_post) |>
    nest(params.posterior = c(alpha, beta)) |>
    mutate(muestras.posterior = map(params.posterior, modelo_beta)) |>
    select(sabor, muestras.posterior)
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/paletas-hist.jpeg :exports results :results output graphics file
  paletas |>
    unnest(muestras.posterior) |>
    ggplot(aes(muestras.posterior)) +
    geom_histogram(aes(fill = sabor), position = "identity", alpha = .8) +
    sin_lineas
#+end_src
#+caption: Histogramas de la distribución predictiva marginal para cada $\theta_j$. 
#+RESULTS:
[[file:../images/paletas-hist.jpeg]]

#+REVEAL: split
#+begin_src R
  ## Utilizamos el metodo Monte Carlo para aproximar la integral. 
  paletas |>
    unnest(muestras.posterior) |>
    mutate(id = rep(seq(1, 5000), 4)) |> group_by(id) |>
    summarise(favorito = sabor[which.max(muestras.posterior)]) |>
    group_by(favorito) |> tally() |>
    mutate(prop = n/sum(n))
#+end_src
#+caption: Aproximación Monte Carlo.
#+RESULTS:
#+begin_src org
   favorito    n   prop
1     fresa  308 0.0616
2 guanábana    1 0.0002
3     limón 1319 0.2638
4     mango 3372 0.6744
#+end_src

#+BEGIN_NOTES
Escencialmente estamos preguntándonos sobre calcular la integral:
\begin{align}
\mathbb{P}(\text{mango sea preferido}) = \int_\Theta f(\theta_1, \ldots, \theta_4) \, p(\theta_1, \ldots, \theta_4 | X_1, \ldots, X_n) d\theta\,,
\end{align}
donde $f(\theta_1, \ldots, \theta_4) = \mathbb{I}_{[\theta_4 \geq \theta_j, j \neq 4]}(\theta_1, \ldots, \theta_4)$. 
#+END_NOTES

** Tarea: Sabores de helados

- ¿Cuál es la probabilidad a priori de que cada sabor sea el preferido?
- Con los datos de arriba, calcula la probabilidad de que la gente prefiera el sabor de mango sobre limón.



* Extensiones: Muestreo por importancia

Incluso cuando tenemos una integral *complicada* podemos ~relajar~ el problema de integración. De tal forma que podemos ~sustituir~
$$\int f(x) \pi(x) \text{d} x = \int f(x) \frac{\pi(x)}{\rho(x)}\,\rho(x) \text{d} x = \int f(x) \, w(x) \, \rho(x) \, \text{d}x\,,$$
donde $\rho$ es una densidad de una variable aleatoria ~adecuada~.

#+REVEAL: split
Esto nos permite utilizar lo que sabemos de las propiedades del método Monte Carlo para resolver la integral
\begin{align*}
\pi(f) =  \int f(x) \pi(x) \text{d} x = \int f(x) \omega(x) \, \rho(x) \, \text{d}x =: \rho(f\omega)\,,
\end{align*}
por medio de una aproximación
\begin{align}
\pi(f) \approx \sum_{n = 1}^{N} \bar{\omega}^{(n)} f(x^{(n)}), \qquad x^{(n)} \overset{\mathsf{iid}}{\sim} \rho\,.
\end{align}
#+REVEAL: split
Al estimador le llamamos el estimador por importancia y lo denotamos por
\begin{align}
\hat{\pi}_N^{\mathsf{IS}}(f) = \sum_{n = 1}^{N} \bar{\omega}^{(n)} f(x^{(n)}), \qquad \bar{\omega}^{(n)} = \frac{\omega(x^{(n)})}{\sum_{m= 1}^{N}\omega(x^{(m)})}\,.
\end{align}

** Propiedades: muestreo por importancia

Lamentablemente, utilizar muestreo por importancia ~impacta la calidad de la
estimación~ (medida, por ejemplo, en términos del *peor error cuadrático medio
cometido*). El impacto es un factor que incorpora la /diferencia/ entre la distribución
~objetivo~ --para integrales de la forma $\int f(x) \text{d}x$, implica la
distribución uniforme-- y la distribución ~sustituto~. Puedes leer más de esto
(aunque a un nivel mas técnico) en la sección 5 de las notas de
citet:Sanz-Alonso2019.

** Ejemplo

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/muestreo-importancia-mezcla.jpeg :exports results :results output graphics file
  crea_mezcla <- function(weights){
    function(x){
      weights$w1 * dnorm(x, mean = -1.5, sd = .5) +
        weights$w2 * dnorm(x, mean = 1.5, sd = .7)
    }
  }
  objetivo <- crea_mezcla(list(w1 = .6, w2 = .4))

  muestras_mezcla <- function(id){
    n <- 100
    tibble(u = runif(n)) |>
      mutate(muestras = ifelse(u <= .6,
                               rnorm(1, -1.5, sd = .5),
                               rnorm(1,  1.5, sd = .7))) |>
      pull(muestras)
  }

  muestras.mezcla <- tibble(id = 1:1000) |>
    mutate(muestras  = map(id, muestras_mezcla)) |>
    unnest(muestras) |>
    group_by(id) |>
    summarise(estimate = mean(muestras))

  g0 <- muestras.mezcla |>
    ggplot(aes(estimate)) +
    geom_histogram() +
    geom_vline(xintercept = -1.5 * .6 + 1.5 * .4,
               lty = 2, color = 'salmon', lwd = 1.5) +
    geom_vline(xintercept = mean(muestras.mezcla$estimate),
               lty = 3, color = 'steelblue', lwd = 1.5) +
    xlim(-1, 1) +
    ggtitle("Objetivo") + sin_lineas

  muestras_uniforme <- function(id){
    n <- 100
    runif(n, -5, 5)
  }

  muestras.uniforme <- tibble(id = 1:1000) |>
    mutate(muestras  = map(id, muestras_uniforme)) |>
    unnest(muestras) |>
    mutate(pix = objetivo(muestras),
           gx  = dunif(muestras, -5, 5),
           wx  = pix/gx) |>
    group_by(id) |>
    summarise(estimate = sum(muestras * wx)/sum(wx))

  g1 <- muestras.uniforme |>
    ggplot(aes(estimate)) +
    geom_histogram() +
    geom_vline(xintercept = -1.5 * .6 + 1.5 * .4,
               lty = 2, color = 'salmon', lwd = 1.5) +
    geom_vline(xintercept = mean(muestras.uniforme$estimate),
               lty = 3, color = 'steelblue', lwd = 1.5) +
    xlim(-1, 1) +
    ggtitle("Uniforme(-5,5)") + sin_lineas

  muestras_importancia <- function(id){
    n <- 100
    rnorm(n, 0, sd = 1)
  }  

  muestras.normal  <- tibble(id = 1:1000) |>
    mutate(muestras  = map(id, muestras_importancia)) |>
    unnest(muestras) |>
    mutate(pix = objetivo(muestras),
           gx  = dnorm(muestras, 0, sd = 1),
           wx  = pix/gx) |>
    group_by(id) |>
    summarise(estimate = sum(muestras * wx)/sum(wx))

  g2  <- muestras.normal |> ggplot(aes(estimate)) +
    geom_histogram() +
    geom_vline(xintercept = -1.5 * .6 + 1.5 * .4,
               lty = 2, color = 'salmon', lwd = 1.5) +
    geom_vline(xintercept = mean(muestras.normal$estimate),
               lty = 3, color = 'steelblue', lwd = 1.5) +
    xlim(-1, 1) +
    ggtitle("Normal(0, 2)") +
    sin_lineas

  g0 + g1 + g2

#+end_src
#+caption: Muestreo por importancia utilizando distintas distribuciones instrumentales. Distribución de muestreo $\pi^{\mathsf{IS}}_N$ con $B = 10,000$ y $n = 100$. 
#+RESULTS:
[[file:../images/muestreo-importancia-mezcla.jpeg]]

#+BEGIN_NOTES
El análisis  del error en la sección anterior habla en del error cuadrático medio en el peor escenario posible bajo una familia de funciones de prueba (resumen). El ejemplo anterior muestra el error Monte Carlo cometido con respecto a una función resumen $f(\theta) = \theta$ con la cual, vemos, se reduce la varianza. Esto no contradice lo anterior pues para esta función resumen nuestra distribución instrumental satisface el criterio de reducción de varianza. En general, lo complicado es encontrar dicha distribución que podamos usar en la estimación Monte Carlo. 
#+END_NOTES


bibliographystyle:abbrvnat
bibliography:references.bib

